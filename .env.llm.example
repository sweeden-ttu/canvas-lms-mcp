# Copy to .env.llm and set values. Used by docker-compose.llm.yml / podman-compose.
# You can create your own image and command; this file overrides the default (Ollama).

# Image for the LLM runner (default: ollama/ollama)
# LLM_IMAGE=ollama/ollama

# Optional command (default: image CMD, e.g. Ollama serve)
# LLM_COMMAND=

# API port (default: 11434; Ollama is OpenAI-compatible at http://localhost:11434)
# LLM_PORT=11434
